{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading, Splitting, and Saving Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/home/roland/Projects/saras_folder/extracted_definitions.csv\")\n",
    "\n",
    "# Stratified split into train+validation and test (80%-20%)\n",
    "train_val_data, test_data = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    stratify=data['definition_sentence'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Splitting train and validation\n",
    "train_data, val_data = train_test_split(\n",
    "    train_val_data,\n",
    "    test_size=0.2,\n",
    "    stratify=train_val_data['definition_sentence'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_data.to_csv(\"train_split_original.csv\", index=False)\n",
    "val_data.to_csv(\"val_split_original.csv\", index=False)\n",
    "test_data.to_csv(\"test_split_original.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 31244\n",
      "Training set size: 19996\n",
      "Validation set size: 4999\n",
      "Test set size: 6249\n",
      "\n",
      "Class distribution in training set:\n",
      "definition_sentence\n",
      "False    18631\n",
      "True      1365\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in validation set:\n",
      "definition_sentence\n",
      "False    4658\n",
      "True      341\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in test set:\n",
      "definition_sentence\n",
      "False    5823\n",
      "True      426\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total dataset size: {len(data)}\")\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_data['definition_sentence'].value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(val_data['definition_sentence'].value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(test_data['definition_sentence'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation of Each Split Independantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This section defines the augmentation methods (synonym replacement and contextual embedding) applied to definitional sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def synonym_augmenter(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    augmented_text = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}:\n",
    "            synonyms = wordnet.synsets(token.text)\n",
    "            if synonyms:\n",
    "                synonym = synonyms[0].lemmas()[0].name()\n",
    "                augmented_text.append(synonym.replace('_', ' '))\n",
    "            else:\n",
    "                augmented_text.append(token.text)\n",
    "        else:\n",
    "            augmented_text.append(token.text)\n",
    "    return \" \".join(augmented_text)\n",
    "\n",
    "contextual_aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
    "\n",
    "# Augmentation\n",
    "def augment_split(data, split_name):\n",
    "    definition_data = data[data['definition_sentence'] == 1]\n",
    "    non_definition_data = data[data['definition_sentence'] == 0]\n",
    "\n",
    "    augmented_sentences = []\n",
    "    for text in tqdm(definition_data['sentence_original'], desc=f\"Augmenting {split_name} definitions\"):\n",
    "    \n",
    "        augmented_sentences.append(contextual_aug.augment(text))\n",
    "        augmented_sentences.append(synonym_augmenter(text))\n",
    "\n",
    "    augmented_definitions = pd.DataFrame({\n",
    "        'sentence_original': augmented_sentences,\n",
    "        'definition_sentence': 1\n",
    "    })\n",
    "\n",
    "    # Combine original and augmented data again\n",
    "    combined_data = pd.concat([non_definition_data, definition_data, augmented_definitions], ignore_index=True)\n",
    "    return combined_data.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Each data split (train-validation-test) is augmented independantly to ensure no data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting train definitions: 100%|██████████| 1365/1365 [06:15<00:00,  3.64it/s]\n",
      "Augmenting validation definitions: 100%|██████████| 341/341 [01:31<00:00,  3.71it/s]\n",
      "Augmenting test definitions: 100%|██████████| 426/426 [01:57<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete. Augmented splits saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = pd.read_csv(\"train_split_original.csv\")\n",
    "val_data = pd.read_csv(\"val_split_original.csv\")\n",
    "test_data = pd.read_csv(\"test_split_original.csv\")\n",
    "\n",
    "# Augment each split indepentantly\n",
    "augmented_train_data = augment_split(train_data, \"train\")\n",
    "augmented_val_data = augment_split(val_data, \"validation\")\n",
    "augmented_test_data = augment_split(test_data, \"test\")\n",
    "\n",
    "# Saving augmented splits\n",
    "augmented_train_data.to_csv(\"train_split_augmented.csv\", index=False)\n",
    "augmented_val_data.to_csv(\"val_split_augmented.csv\", index=False)\n",
    "augmented_test_data.to_csv(\"test_split_augmented.csv\", index=False)\n",
    "\n",
    "print(\"Augmentation complete. Augmented splits saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentated Data Validation: Class Distribution and Checking Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of augmented training data: 22726\n",
      "Size of augmented validation data: 5681\n",
      "Size of augmented test data: 7101\n",
      "\n",
      "Class distribution in augmented training data:\n",
      "definition_sentence\n",
      "0    18631\n",
      "1     4095\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in augmented validation data:\n",
      "definition_sentence\n",
      "0    4658\n",
      "1    1023\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in augmented test data:\n",
      "definition_sentence\n",
      "0    5823\n",
      "1    1278\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load augmented splits\n",
    "augmented_train_data = pd.read_csv(\"/home/roland/Projects/saras_folder/train_split_augmented.csv\")\n",
    "augmented_val_data = pd.read_csv(\"/home/roland/Projects/saras_folder/val_split_augmented.csv\")\n",
    "augmented_test_data = pd.read_csv(\"/home/roland/Projects/saras_folder/test_split_augmented.csv\")\n",
    "\n",
    "# Print sizes of each augmented split\n",
    "print(f\"Size of augmented training data: {len(augmented_train_data)}\")\n",
    "print(f\"Size of augmented validation data: {len(augmented_val_data)}\")\n",
    "print(f\"Size of augmented test data: {len(augmented_test_data)}\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(\"\\nClass distribution in augmented training data:\")\n",
    "print(augmented_train_data['definition_sentence'].value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in augmented validation data:\")\n",
    "print(augmented_val_data['definition_sentence'].value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in augmented test data:\")\n",
    "print(augmented_test_data['definition_sentence'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
